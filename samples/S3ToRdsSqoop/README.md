# Loading a CSV file stored in S3 into an RDS MySQL instance

This sample uses [sqoop](http://sqoop.apache.org/) to load a CSV filed stored in [S3](https://aws.amazon.com/s3/) into a [MySQL](https://www.mysql.com/) or [MariaDB](https://mariadb.org/) database instance managed by [RDS](https://aws.amazon.com/rds/). Sqoop is a specialized tool that uses [hadoop](http://hadoop.apache.org/) to transfer bulk data in and out of relational databases. It completes this task more quickly than Data Pipeline's built-in CopyActivity, but it is also more resource intensive. The sample takes advantage of built-in support for sqoop in [EMR](https://aws.amazon.com/emr/) 5.0.

## Parameters

Parameter | Required | Description
----------|----------|------------
myEmrMasterInstanceType | no | The EC2 instance type to use for the master node in the EMR cluster. Default: m2.xlarge
myEmrCoreInstanceType | no | The EC2 instance type to use for the core nodes in the EMR cluster. Default: m2.xlarge
myEmrCoreInstanceCount | no | The number of core nodes to launch in the EMR cluster. Default: 2
myRdsEndpoint | yes | DNS endpoint for target RDS instance. The value should include the port number. Example: test.xyzw.us-east-1.rds.amazonaws.com:3306
myRdsDatabaseName | yes | Name of the target MySQL or MariaDB database.
myRdsTableName | yes | Name of the database table that the CSV will be imported into.
myRdsUsername | yes | User name to use to connect to RDS.
\*myRdsPassword | yes | Password to use to connect to RDS.
myS3InputDataLocation | yes | S3 path to folder where the CSV data is stored. Example: s3://example-s3-path/folder-containing-csv-data/
myPipelineLogUri | yes | S3 folder where log data generated by this pipeline will be written. Example: s3::/example-s3-path/folder-to-contain-log-files/

## Prerequisites

This template assumes that you have already created an RDS instance running MySQL or MariaDB instance in RDS. Inside the instance you will need a database and table where the records will be inserted. You will need to know the database name, the table name, the database user name and password, and the DNS endpoint of the RDS instance. You can use the RDS console to view the DNS endpoint and the master user name and to modify the master password as needed. You will need to use the MySQL command-line tool or a graphical client like [MySQL Workbench](https://www.mysql.com/products/workbench/) to create the target database and table. See [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToInstance.html) for more information on connecting to MySQL on RDS. Note that the schema of the table where you will be importing records should match the schema in the CSV file (i.e., it should have the same number of columns and appropriate column types).

## Running this sample

Create a new pipeline. Throughout this section we assume that the S3ToRdsSqoop sample directory is
your current working directory.

```sh
 $> aws datapipeline create-pipeline --name s3-to-rds-sqoop --unique-id s3-to-rds-sqoop
# {
#     "pipelineId": "df-03971252U4AVY60545T7"
# }
```

Upload the [pipeline definition](http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-writing-pipeline-definition.html). Use the `pipelineId` that was returned by the `create-pipeline` command. Specify the required parameters.

```sh
  $> aws datapipeline put-pipeline-definition --pipeline-id <your pipelineId> \
    --pipeline-definition file://sqoop_activity.json \
    --parameter-values myRdsEndpoint=<RDS endpoing> myRdsDatabaseName=<database name> myRdsTableName=<table name> \
    myRdsUsername=<user name> '*myRdsPassword=<password>' myS3InputDataLocation=<data location> myPipelineLogUri=<log location>
# {
#     "errored": false,
#     "validationWarnings": [],
#     "validationErrors": []
# }
```

Activate the pipeline. Use the `pipelineId` that was returned by the `create-pipeline` command.

```sh
  $> aws datapipeline activate-pipeline --pipeline-id <your pipelineId>
```

Optionally, check the status of your running pipeline. Use the `pipelineId` that was returned by the
`create-pipeline` command. When the pipeline has completed, the Status Ended column in the output
from this command will show FINISHED for all pipeine nodes.

```sh

  >$ aws datapipeline list-runs --pipeline-id <your pipelineId>

```

Once the pipeline has completed, you should be able to see the imported records in MySQL by running a SELECT query using the MySQL command-line tool or a graphical client.

## Next steps

In addition to the required parameters, there are optional parameters to set the EC2 instance types launched by the EMR cluster as well as the number of core nodes to launch. Changing these paramters may improve the performance of the import job.

Once the pipeline is completed, you can delete it with the following command.

```sh
 $> aws datapipeline delete-pipeline --pipeline-id <your pipelineId>
```

The resources used by this example will incur normal charges. If you created any resources specifically to test this pipeline, you may wish to delete them now.

## Disclaimer

The samples in this repository are meant to help users get started with Data Pipeline. They may not
be sufficient for production environments. Users should carefully inspect samples before running
them.

*Use at your own risk.*

Licensed under the MIT-0 License.
